{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 1. 트랜스포머 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 인코더-디코더 프레임워크"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `RNN (Recurrent Neural Network)`\n",
    "  - (단어 또는 문자와 같은) 입력 → RNN 셀 → `은닉 상태(Hidden State)` *벡터* 출력\n",
    "    - RNN에서는 신경망의 층을 셀(Cell)이라고 부름\n",
    "  - 동시에 출력 정보를 `피드백 루프(Feedback Loop)`로 보내 자기 자신에 다시 입력\n",
    "    - 출력 정보의 일부를 다음 스텝에 사용\n",
    "  - 각 스텝에서 상태 정보를 시퀀스의 다음 작업으로 전달\n",
    "    - 이런 식으로 이전 스텝의 정보를 추적 및 예측 수행\n",
    "  - 예) 기계 번역: 단어 시퀀스를 한 언어에서 다른 언어로 매핑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `인코더-디코더(Encoder-Decoder)` 또는 `시퀀스-투-시퀀스(Sequence-to-Sequence`)\n",
    "  - 입력과 출력이 임의의 길이를 가진 시퀀스일 때 잘 맞음\n",
    "  - 인코더: 입력 시퀀스의 정보를 `마지막 은닉 상태`(수치 표현)로 인코딩\n",
    "  - 디코더: 이 상태가 디코더로 전달되어 출력 시퀀스 생성\n",
    "  - 약점\n",
    "    - 인코더-디코더 구조는 마지막 은닉 상태가 `정보 병목(Information Bottleneck)`이 된다.\n",
    "      - 디코더는 인코더의 마지막 은닉 상태만을 참조 및 출력 생성. 여기에 전체 시퀀스의 의미가 담겨야 함\n",
    "      - → 시퀀스가 긴 경우, 모든 것을 하나의 표현으로 압축하는 과정에서 시작 부분의 정보가 손실될 가능성이 있음\n",
    "      - ⇒ `Long-term Dependency` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `어텐션(Attention)`\n",
    "  - 인코더-디코더의 Long-term Dependency를 해결\n",
    "  - 디코더가 인코더의 *모든 은닉 상태에 접근*하여 이 병목을 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 어텐션 메커니즘\n",
    "- `어텐션`\n",
    "  - 입력 시퀀스에서 마지막 하나의 은닉 상태만을 만들지 않고, 타임스텝마다 인코더에서 디코더가 참고할 은닉 상태를 출력\n",
    "  - 모든 상태를 동시에 사용하려면 디코더에 많은 입력이 발생 → 어떤 상태를 먼저 사용할지 우선순위를 정함\n",
    "    - 디코더가 모든 디코딩 타임스텝마다 인코더의 각 상태에 다른 가중치 또는 `어텐션`을 할당\n",
    "  - 어텐션 기반 모델은 타임스텝마다 가장 많이 관련된 입력 토근에 초점을 맞추므로 번역 문장에 있는 단어와 원 문장에 있는 단어의 복잡한 정렬 문제를 학습한다.\n",
    "- `셀프 어텐션(Self-attention)` = `트랜스포머`\n",
    "  - 인코더-디코더의 순환 모델은 태생적으로 계산이 순차적으로 수행되며 입력 시퀀스 전체에 병렬화를 할 수 없다.\n",
    "  - → 이에 순환을 없앤 `셀프 어텐션(Self-attention)`\n",
    "    - 인코더와 디코더에 각각 셀프 어텐션 존재\n",
    "    - 어텐션의 출력은 피드 포워드 신경망(FF NN, Feed-Forward Neural Network)에 주입"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 NLP의 전이 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `사전 훈련`\n",
    "- `도메인 적응`\n",
    "- `미세 튜닝`\n",
    "- 셀프 어텐션과 전이 학습을 결합한 `트랜스포머` 모델 공개 (2018년)\n",
    "  - `GPT`\n",
    "    - 트랜스포머 아키텍처의 디코더 부분만 사용\n",
    "    - ULMFiT 같은 언어 모델링 방법 사용\n",
    "  - `BERT`\n",
    "    - 트랜스포머 아키텍처의 인코더 부분만 사용\n",
    "    - `마스크드 언어 모델링(Masked Langauge Modeling)` 사용\n",
    "      - 텍스트에서 랜덤하게 마스킹된 단어를 예측하는 것이 목표\n",
    "      - 예) 'I looked at my [MASK] and swa that [MASK] was late.'에서, 모델은 [MASK]로 마스킹된 단어에 대해 가장 가능성이 높은 후보를 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 허깅페이스 트랜스포머스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 트랜스포머 애플리케이션 둘러보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Dear Amazon, last week I ordered an Optimus Prime action figure \\\n",
    "    from your online store in Germany. Unfortunately, when I opened the package, \\\n",
    "    discovered to my horror that I had been sent an action figure of Megatron \\\n",
    "    instead! As a lifelong enemy of the Decepticons, I hope you can understand my \\\n",
    "    dilemma. To resolve the issue, I demand an exchange of Megatron for the \\\n",
    "    Optimus Prime figure I ordered. Enclosed are copies of my records concerning \\\n",
    "    this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1 텍스트 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e0b1e2ca0f419296960f233c1cbfcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe38b0fb8039418d9ed0467384afe6a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47276bfbbbf3406eb89b195f3247e89a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3269950c46cb4f47ae01da0dcf4e5df6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlee/anaconda3/envs/py38-torch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.785798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label     score\n",
       "0  NEGATIVE  0.785798"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "classifier = pipeline(\"text-classification\")\n",
    "\n",
    "outputs = classifier(text)\n",
    "pd.DataFrame(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2 개체명 인식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.3 질문 답변"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.4 요약"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.5 번역"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.6 텍스트 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 허깅페이스 생태계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.1 허깅페이스 허브"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.2 허깅페이스 토크나이저"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.3 허깅페이스 데이터셋"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.4 허깅페이스 액셀러레이트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 트랜스포머의 주요 도전 과제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 언어\n",
    "- 데이터 가용성\n",
    "- 긴 문서 처리하기\n",
    "- 불투명성\n",
    "- 편향"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
