{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 1 신경망 복습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 수학과 파이썬 복습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 백터와 행렬\n",
    "> 💡 벡터\n",
    "- 벡터는 크기와 방향을 가진 양입니다. \n",
    "- 벡터는 숫자가 일렬로 늘어선 집합으로 표현할 수 있으며, 파이썬에서는 1차원 배열로 취급할 수 있습니다.\n",
    "\n",
    "> 💡 행렬\n",
    "- 행렬은 숫자가 2차원 형태(사각형 형상)로 늘어선 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(3,)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([1, 2, 3])\n",
    "print(x.__class__)\n",
    "print(x.shape)\n",
    "print(x.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 💡 numpy.ndarray\n",
    "- N-dimensional Array의 줄임말. 다차원 배열"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 💡 `x = np.array([1, 2, 3])`일 때, `x`는 열벡터인가? 행백터인가?\n",
    "\n",
    "- `x = np.array([1, 2, 3])`의 경우, `x`는 1차원 배열입니다. 이 배열은 특정한 방향성을 가지지 않으며, 행벡터도 열벡터도 아닙니다. \n",
    "\n",
    "- 행벡터 또는 열벡터로 간주하려면 2차원 배열로 표현되어야 합니다:\n",
    "\n",
    "  - **행벡터**: `x = np.array([[1, 2, 3]])`  → 이 경우, `x`는 1행 3열의 2차원 배열로, 행벡터입니다.\n",
    "  - **열벡터**: `x = np.array([[1], [2], [3]])`  → 이 경우, `x`는 3행 1열의 2차원 배열로, 열벡터입니다.\n",
    "\n",
    "- 따라서 `x = np.array([1, 2, 3])`는 1차원 배열이며, 행벡터나 열벡터로 특정 방향성을 가지지 않는다고 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(1, 3)\n",
      "2\n",
      "<class 'numpy.ndarray'>\n",
      "(3, 1)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y = np.array([[1, 2, 3]])\n",
    "print(y.__class__)\n",
    "print(y.shape)\n",
    "print(y.ndim)\n",
    "\n",
    "z = np.array([[1], [2], [3]])\n",
    "print(z.__class__)\n",
    "print(z.shape)\n",
    "print(z.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(2, 3)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "W = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(W.__class__)\n",
    "print(W.shape)\n",
    "print(W.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 💡 행렬 vs. 배열\n",
    "\n",
    "- 주요 차이점 요약\n",
    "  - 차원: 배열은 다차원 데이터를 다룰 수 있는 반면, 행렬은 항상 2차원입니다.\n",
    "  - 연산: 배열은 원소 단위 연산을 지원하며, 행렬은 선형 대수학에서 사용되는 특정 연산을 수행할 수 있습니다.\n",
    "  - 사용 방법: NumPy에서 배열은 np.array를 주로 사용하며, 행렬은 np.matrix로 표현할 수 있으나, 현재는 np.array를 사용하는 것이 더 권장됩니다.\n",
    "\n",
    "- 따라서, 모든 행렬은 2차원 배열이지만, 모든 배열이 행렬은 아닙니다. NumPy에서는 일반적으로 np.array를 사용하여 배열 및 행렬을 처리하는 것이 표준입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 2차원 배열 (2x3)\n",
    "arr = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(arr)\n",
    "\n",
    "# 2차원 행렬 (2x3)\n",
    "mat = np.matrix([[1, 2, 3], [4, 5, 6]])\n",
    "print(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 행렬의 원소별 연산\n",
    "- 원소별(Element-Wise) 연산: + (더하기), * (곱하기)\n",
    "- 넘파이의 다차원 배열에서는 서로 대응하는 원소끼리(각 원소가 독립적으로) 연산이 이뤄집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  3  5]\n",
      " [ 7  9 11]]\n",
      "[[ 0  2  6]\n",
      " [12 20 30]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "W = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "X = np.array([[0, 1, 2], [3, 4, 5]])\n",
    "\n",
    "print(W + X)\n",
    "print(W * X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 브로드캐스트\n",
    "- 넘파이의 다차원 배열에서는 형상이 다른 배열끼리도 연산할 수 있습니다.\n",
    "  - 예) 스칼라 값 10이 2×2 행렬로 확장된 후에 원소별 연산을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10 20]\n",
      " [30 40]]\n",
      "[[10 40]\n",
      " [30 80]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "print(A * 10)\n",
    "\n",
    "b = np.array([10, 20])\n",
    "print(A * b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 벡터의 내적과 행렬의 곱\n",
    "> 💡 백터의 내적\n",
    "- 백터의 내적은 직관적으로 '두 벡터가 얼마나 같은 방향을 향하고 있는가'를 나타냅니다. 벡터의 길이가 1인 경우로 한정하면, 완전히 같은 방향을 원하는 두 벡터의 내적은 1이 됩니다. 반대로, 반대 방향을 향하는 두 벡터의 내적은 -1입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "[[19 22]\n",
      " [43 50]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 벡터의 내적\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "print(np.dot(a, b))\n",
    "\n",
    "# 행렬의 곱\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "print(np.matmul(A, B))  # np.dot()도 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `np.dot(x, y)`\n",
    "- 인수 x, y가 모두 1차원 배열이면 벡터의 내적을 계산하고, 2차원 배열이면 행렬의 곱을 계산합니다.\n",
    "- 다만, 둘을 구분하여 코드의 논리와 의도를 명확히 하는 것이 좋습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.5 행렬 형상 확인\n",
    "- 행렬의 곱에서는 대응하는 차원의 원소 수를 일치시킵니다.\n",
    "- 행렬의 곱 등 행렬을 계산할 때는 형상 확인이 중요합니다. 그래야 신경망 구현을 원활히 진행할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 신경망의 추론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 신경망 추론 전체 그림\n",
    "- 입력층\n",
    "- 출력층\n",
    "- 은닉층(혹은 중간층)\n",
    "- 가중치\n",
    "- 편향"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 미니배치\n",
    "  - 전체 데이터를 작은 그룹으로 나눠 그룹 단위로 반복 학습하는 방식을 미니배치 학습이라고 하며, 이 때 각각의 그룹을 미니배치라고 합니다. 이 책에서의 신경만 학습은 기본적으로 미니배치 방식을 가정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4)\n",
      "(4,)\n",
      "(10, 2)\n",
      "(10, 4)\n"
     ]
    }
   ],
   "source": [
    "# 완전연결계층에 의한 변환(미니배치 버전)\n",
    "import numpy as np\n",
    "\n",
    "x = np.random.randn(10, 2)  # 입력\n",
    "W1 = np.random.randn(2, 4)  # 가중치\n",
    "b1 = np.random.randn(4)  # 편향\n",
    "h = np.matmul(x, W1) + b1  # 은닉층(중간층) 뉴런 계산\n",
    "\n",
    "print(x.shape)\n",
    "print(W1.shape)\n",
    "print(b1.shape)\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 활성화 함수\n",
    "  - 완전연결계층에 의한 변환은 선형 변환입니다.\n",
    "  - 여기에 '비선형' 효과를 부여하는 것이 활성화 함수입니다.\n",
    "  - (예) 시그모이드 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2)\n",
      "(2, 4)\n",
      "(4,)\n",
      "(10, 4)\n",
      "(10, 4)\n",
      "(4, 3)\n",
      "(3,)\n",
      "(10, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# 입력: 2차원 데이터 10개가 미니배치로 처리\n",
    "x = np.random.randn(10, 2)\n",
    "\n",
    "W1 = np.random.randn(2, 4)\n",
    "b1 = np.random.randn(4)\n",
    "W2 = np.random.randn(4, 3)\n",
    "b2 = np.random.randn(3)\n",
    "h = np.matmul(x, W1) + b1\n",
    "\n",
    "a = sigmoid(h)\n",
    "\n",
    "# 출력: 10개의 데이터가 한꺼번에 처리됨. 각 데이터는 3차원 데이터로 변환됨.\n",
    "s = np.matmul(a, W2) + b2\n",
    "\n",
    "print(x.shape)\n",
    "print(W1.shape)\n",
    "print(b1.shape)\n",
    "print(h.shape)\n",
    "print(a.shape)\n",
    "print(W2.shape)\n",
    "print(b2.shape)\n",
    "print(s.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 계층으로 클래스화 및 순전파 구현\n",
    "- 완전연결계층에 의한 변환을 Affine 계층으로, 시그모이드 함수에 의한 변환을 Sigmoid 계층으로 구현합니다.\n",
    "  - 완전연결계층에 의한 변환은 기하학에서 아핀 변환에 해당합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.matmul(x, W) + b\n",
    "        return out\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        I, H, O = input_size, hidden_size, output_size\n",
    "\n",
    "        # 가중치와 편향 초기화\n",
    "        W1 = np.random.randn(I, H)\n",
    "        b1 = np.random.randn(H)\n",
    "        W2 = np.random.randn(H, O)\n",
    "        b2 = np.random.randn(O)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = [Affine(W1, b1), Sigmoid(), Affine(W2, b2)]\n",
    "\n",
    "        # 모든 가중치를 리스트에 모은다.\n",
    "        self.params = []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.34769252 -0.39492585  1.56840823]\n",
      " [ 0.40525405 -0.14867791  1.59890539]\n",
      " [ 0.3989313  -0.35200165  1.69127273]\n",
      " [ 0.94850369  1.08024514  1.30723339]\n",
      " [ 0.44432485 -0.61781224  1.72676679]\n",
      " [ 0.42973902 -0.23655385  1.69097698]\n",
      " [ 0.42578109 -0.25784763  1.69733645]\n",
      " [ 1.06414622  1.0447757   1.66212856]\n",
      " [ 0.74762624  0.55157775  1.44404279]\n",
      " [ 0.48026131  0.05894612  1.55271086]]\n",
      "(10, 3)\n",
      "[array([[-1.06517443,  0.23101624,  0.29325999,  0.18701739],\n",
      "       [-0.24904366, -0.43950346,  1.08435185,  1.86141432]]), array([-0.64569294,  1.52973795,  0.08057603, -0.20458148]), array([[ 0.12545946, -0.27430325,  0.59574434],\n",
      "       [ 0.06824406, -0.99281527, -0.16524494],\n",
      "       [-0.59681546,  0.24804832,  0.85713583],\n",
      "       [ 1.16836967,  1.25874874, -0.87010314]]), array([0.31936984, 0.46997656, 1.44892981])]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(10, 2)\n",
    "model = TwoLayerNet(2, 4, 3)\n",
    "s = model.predict(x)\n",
    "\n",
    "print(s)\n",
    "print(s.shape)\n",
    "\n",
    "print(model.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 신경망의 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 손실 함수\n",
    "- 손실: 학습 데이터(학습 시 주어진 정답데이터)와 신경망이 예측한 결과를 비교하여 예측이 얼마나 나쁜가를 산출한 단일 값(스칼라)\n",
    "- 손실 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 소프트맥스 함수\n",
    "  - 분자: 점수 $s_k$의 지수 함수\n",
    "  - 분모: 모든 입력 신호의 지수 함수의 총합\n",
    "  - 소프트맥스 함수의 출력의 각 원소는 0.0 이상 1.0 이하의 실수이며 그 원소들을 모두 더하면 1.0이 됩니다. → 확률로 해석\n",
    "  - 출력이 총 $n$개일 때, $k$번째의 출력 $y_k$를 구하는 계산식\n",
    "$$\n",
    "y_k = \\frac{\\exp(s_k)}{\\sum_{i=1}^n\\exp(s_i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 💡 소프트맥스 함수\n",
    "\n",
    "$$\n",
    "\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\n",
    "$$\n",
    "\n",
    "여기서,\n",
    "- $s_k$는 입력 벡터 $s$의 $k$번째 요소\n",
    "- $n$는 입력 벡터의 전체 요소 수, 즉 클래스의 수\n",
    "- $e^x = \\exp(x)$. $e$는 자연로그의 밑수(약 2.718)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예를 들어,  z = [2.0, 1.0, 0.1] 이라는 벡터가 있을 때, 소프트맥스 함수는 이 벡터를 다음과 같이 변환합니다:\n",
    "\n",
    "$$\n",
    "\\text{Softmax}(z) = \\left[ \\frac{e^{2.0}}{e^{2.0} + e^{1.0} + e^{0.1}}, \\frac{e^{1.0}}{e^{2.0} + e^{1.0} + e^{0.1}}, \\frac{e^{0.1}}{e^{2.0} + e^{1.0} + e^{0.1}} \\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 💡 (소프트맥스 함수에서) 지수 연산을 수행하는 이유는?\n",
    "- 입력 값들 사이의 차이를 강조하여 확률 값을 더 명확하게 구분하기 위함입니다.\n",
    "- 이를 통해 모델이 더 자신 있게 특정 클래스에 높은 확률을 부여할 수 있도록 합니다.\n",
    "- 구체적인 이유\n",
    "  - 비선형성 도입\n",
    "    - 지수 함수 $e^x$는 비선형 함수로, 입력 값의 크기가 커질수록 출력 값이 매우 빠르게 증가합니다. 이로 인해 소프트맥스 함수는 입력 값들 사이의 작은 차이도 크게 확대하여 표현할 수 있습니다.\n",
    "    - 따라서 큰 값을 가진 입력이 더 큰 확률 값을 갖게 되어, 확률 분포가 더 뚜렷해집니다.\n",
    "  - 확률 분포 형성\n",
    "    - 소프트맥스 함수의 입력 값들의 지수 연산 결과를 정규화(Normalize)하여 각 클래스에 속할 확률로 변환합니다.\n",
    "    - 지수 연산을 통해 각 값의 차이를 극대화한 뒤, 정규화를 통해 총합이 1이 되는 확률 분포를 형성합니다.\n",
    "  - 순서 유지\n",
    "    - 지수 함수는 단조 증가 함수이므로, 입력 값의 순서를 유지합니다.\n",
    "    - 즉, 만약 $z_1 > z_2$라면, 소프트맥스 함수 적용 후에도 $\\text{Softmax}(z_1) > \\text{Softmax}(z_2)$가 유지됩니다.\n",
    "    - 이는 클래스 간의 상대적 순서를 그대로 반영하면서도 그 차이를 강조하는 효과를 줍니다.\n",
    "  - 요약\n",
    "    - 지수 연산은 입력 값의 크기 차이를 확대해주며, \n",
    "    - 이 확대된 값을 바탕으로 확률을 계산함으로써 소프트맥스 함수가 클래스 간의 확률을 더 명확히 구분할 수 있게 합니다. \n",
    "    - 이를 통해 모델은 어느 클래스가 가장 가능성이 높은지 더 확실하게 판단할 수 있게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 미분과 기울기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 연쇄 법칙"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4 계산 그래프"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.5 기울기 도출과 역전파 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.6 가중치 갱신"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noname",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
